{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-aPBZaSdr6n"
   },
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2934,
     "status": "ok",
     "timestamp": 1578726561399,
     "user": {
      "displayName": "ちんちゃんねるAIエンジニア",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDAp76sYQh20e_nmKms9FsgMnK6Wln1dGNxxPr_=s64",
      "userId": "02172563712230456611"
     },
     "user_tz": -540
    },
    "id": "LRBAKUnAySCz",
    "outputId": "068b208f-5f18-417f-d946-82d177e2b2ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYaKx/vUlvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0].reshape(8, 8))\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2929,
     "status": "ok",
     "timestamp": 1578726561399,
     "user": {
      "displayName": "ちんちゃんねるAIエンジニア",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDAp76sYQh20e_nmKms9FsgMnK6Wln1dGNxxPr_=s64",
      "userId": "02172563712230456611"
     },
     "user_tz": -540
    },
    "id": "LZLwmKx8yTha",
    "outputId": "d2293808-ba12-43f8-f5b4-c6cc0b979bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1sdeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[1].reshape(8, 8))\n",
    "print(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2924,
     "status": "ok",
     "timestamp": 1578726561400,
     "user": {
      "displayName": "ちんちゃんねるAIエンジニア",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDAp76sYQh20e_nmKms9FsgMnK6Wln1dGNxxPr_=s64",
      "userId": "02172563712230456611"
     },
     "user_tz": -540
    },
    "id": "jDoiGvjId7PT",
    "outputId": "ad1644b7-3a12-42f8-8b55-ab85904c9996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n",
      "[6 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "print(model.predict(X_test[:2])) # X_test[0]とX_test[1]を推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13439,
     "status": "ok",
     "timestamp": 1578726571921,
     "user": {
      "displayName": "ちんちゃんねるAIエンジニア",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDAp76sYQh20e_nmKms9FsgMnK6Wln1dGNxxPr_=s64",
      "userId": "02172563712230456611"
     },
     "user_tz": -540
    },
    "id": "MvWGW70welTP",
    "outputId": "7085c868-2a78-4a67-ad6c-95b71156dcd7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 9,610\n",
      "Trainable params: 9,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1347 samples\n",
      "Epoch 1/100\n",
      "1347/1347 [==============================] - 0s 217us/sample - loss: 2.2608 - acc: 0.4959\n",
      "Epoch 2/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.3856 - acc: 0.8834\n",
      "Epoch 3/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.2122 - acc: 0.9347\n",
      "Epoch 4/100\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.1461 - acc: 0.9577\n",
      "Epoch 5/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.1122 - acc: 0.9696\n",
      "Epoch 6/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0834 - acc: 0.9822\n",
      "Epoch 7/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0680 - acc: 0.9852\n",
      "Epoch 8/100\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0579 - acc: 0.9889\n",
      "Epoch 9/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0451 - acc: 0.9911\n",
      "Epoch 10/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0381 - acc: 0.9955\n",
      "Epoch 11/100\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0330 - acc: 0.9941\n",
      "Epoch 12/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0305 - acc: 0.9948\n",
      "Epoch 13/100\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0254 - acc: 0.9955\n",
      "Epoch 14/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0211 - acc: 0.9978\n",
      "Epoch 15/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0190 - acc: 0.9978\n",
      "Epoch 16/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0192 - acc: 0.9970\n",
      "Epoch 17/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0149 - acc: 0.9985\n",
      "Epoch 18/100\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0143 - acc: 0.9985\n",
      "Epoch 19/100\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0113 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0109 - acc: 0.9993\n",
      "Epoch 21/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0101 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 9.8728e-04 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 9.4346e-04 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 9.3692e-04 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 9.0000e-04 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 8.3672e-04 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 8.0006e-04 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 7.8045e-04 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 7.4831e-04 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 7.1683e-04 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 6.7783e-04 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 6.9604e-04 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 6.3216e-04 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 6.3073e-04 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1347/1347 [==============================] - 0s 62us/sample - loss: 5.8715e-04 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 5.8127e-04 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 5.4738e-04 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 5.3643e-04 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 5.0604e-04 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 4.9077e-04 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 5.0002e-04 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 4.5802e-04 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 4.4759e-04 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 4.5769e-04 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 9.1934e-04 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 4.9977e-04 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 4.1576e-04 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 3.7807e-04 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 3.6122e-04 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 3.4485e-04 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 3.3468e-04 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 3.1810e-04 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 3.0885e-04 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 2.9808e-04 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 2.9013e-04 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 2.7791e-04 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1347/1347 [==============================] - 0s 61us/sample - loss: 2.7196e-04 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1347/1347 [==============================] - 0s 60us/sample - loss: 2.6111e-04 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 2.5598e-04 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 2.4820e-04 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 2.3945e-04 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 2.3205e-04 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 2.2771e-04 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 2.1906e-04 - acc: 1.0000\n",
      "450/450 [==============================] - 0s 69us/sample - loss: 0.0904 - acc: 0.9800\n",
      "0.98\n",
      "[6 9]\n"
     ]
    }
   ],
   "source": [
    "# keras\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(64,)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "# sparse_categorical_crossentropy vs categorical_crossentropy\n",
    "# https://jovianlin.io/cat-crossentropy-vs-sparse-cat-crossentropy/\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print(acc)\n",
    "pred = model.predict(X_test[:2])\n",
    "print(np.argmax(pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UVxojRIH45SS"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myMMzLU-2BA-"
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "  def __init__(self, W, b):\n",
    "    self.W = W\n",
    "    self.b = b\n",
    "    \n",
    "    self.x = None\n",
    "    self.original_x_shape = None\n",
    "\n",
    "    self.dW = None\n",
    "    self.db = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    self.original_x_shape = x.shape\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    self.x = x\n",
    "\n",
    "    out = np.dot(self.x, self.W) + self.b\n",
    "    return out\n",
    "\n",
    "  def backword(self, dout):\n",
    "    dx = np.dot(dout, self.W.T)\n",
    "    self.dW = np.dot(self.x.T, dout)\n",
    "    self.db = np.sum(dout, axis=0)\n",
    "\n",
    "    # print(\"affine dx before:\" + str(dx.shape))\n",
    "    dx = dx.reshape(*self.original_x_shape)\n",
    "    # print(\"affine dx after:\" + str(dx.shape))\n",
    "    return dx\n",
    "\n",
    "class Relu:\n",
    "  def __init__(self):\n",
    "    self.mask = None\n",
    "  \n",
    "  def forward(self, x):\n",
    "    self.mask = (x <= 0)\n",
    "    out = x.copy()\n",
    "    out[self.mask] = 0\n",
    "    return out\n",
    "\n",
    "  def backword(self, dout):\n",
    "    dout[self.mask] = 0\n",
    "    dx = dout\n",
    "    return dx\n",
    "\n",
    "class Sigmoid:\n",
    "  def __init__(self):\n",
    "    self.out = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = 1 / (1 + np.exp(-x))\n",
    "    self.out = out\n",
    "    return out\n",
    "\n",
    "  def backword(self, dout):\n",
    "    dx = dout * self.out * (1 - self.out)\n",
    "    return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "  def __init__(self):\n",
    "    self.loss = None\n",
    "    self.y = None\n",
    "    self.t = None\n",
    "\n",
    "  def forward(self, x, t):\n",
    "    self.t = t\n",
    "    self.y = softmax(x)\n",
    "    self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "    return self.loss\n",
    "\n",
    "  def backword(self, dout=1):\n",
    "    # doutを使っていない\n",
    "    batch_size = self.t.shape[0]\n",
    "    if self.t.size == self.y.size:\n",
    "      # print(\"same size\")\n",
    "      dx = (self.y - self.t) / batch_size\n",
    "    else:\n",
    "      # print(\"not same size\")\n",
    "      # print(\"t size:\" + str(self.t.size))\n",
    "      # print(\"x size:\" + str(self.y.size))\n",
    "      dx = self.y.copy()\n",
    "      dx[np.arange(batch_size), self.t] -= 1\n",
    "      dx = dx / batch_size\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCzWGeWbemSG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "    self.params = {}\n",
    "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "    self.params['b1'] = np.zeros(hidden_size)\n",
    "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    self.layers = OrderedDict()\n",
    "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "    self.layers['Relue1'] = Relu()\n",
    "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "    self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "  def predict(self, x):\n",
    "    for layer in self.layers.values():\n",
    "      x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "  def loss(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    return self.lastLayer.forward(y, t)\n",
    "\n",
    "  def accuracy(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "    return accuracy\n",
    "\n",
    "  def numerical_gradient(self, x, t):\n",
    "    loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "    grads = {}\n",
    "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "    return grads\n",
    "\n",
    "  def gradient(self, x, t):\n",
    "    self.loss(x, t)\n",
    "\n",
    "    dout = 1\n",
    "    dout = self.lastLayer.backword(dout)\n",
    "\n",
    "    layers = list(self.layers.values())\n",
    "    layers.reverse()\n",
    "    for layer in layers:\n",
    "      dout = layer.backword(dout)\n",
    "\n",
    "    grads = {}\n",
    "    grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "    grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2013,
     "status": "ok",
     "timestamp": 1578727045603,
     "user": {
      "displayName": "ちんちゃんねるAIエンジニア",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDAp76sYQh20e_nmKms9FsgMnK6Wln1dGNxxPr_=s64",
      "userId": "02172563712230456611"
     },
     "user_tz": -540
    },
    "id": "iflmSt7y7Yb_",
    "outputId": "f1768112-578c-4b44-8e13-c0cb66cefb3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "====train acc: 0.214550853749072 test acc: 0.19777777777777777 ====\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "====train acc: 0.36525612472160357 test acc: 0.3333333333333333 ====\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "====train acc: 0.23311061618411286 test acc: 0.20666666666666667 ====\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "====train acc: 0.5701559020044543 test acc: 0.5088888888888888 ====\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "====train acc: 0.5530809205642168 test acc: 0.5577777777777778 ====\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "====train acc: 0.5478841870824054 test acc: 0.5244444444444445 ====\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "====train acc: 0.726800296956199 test acc: 0.7288888888888889 ====\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "====train acc: 0.8648849294729027 test acc: 0.8755555555555555 ====\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "====train acc: 0.7401633259094283 test acc: 0.7644444444444445 ====\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "====train acc: 0.8507795100222717 test acc: 0.8533333333333334 ====\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "====train acc: 0.8426132145508537 test acc: 0.8466666666666667 ====\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "====train acc: 0.9242761692650334 test acc: 0.9311111111111111 ====\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "====train acc: 0.9569413511507052 test acc: 0.9422222222222222 ====\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "====train acc: 0.9524870081662955 test acc: 0.9377777777777778 ====\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "====train acc: 0.9643652561247216 test acc: 0.9555555555555556 ====\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "====train acc: 0.9628804751299184 test acc: 0.9577777777777777 ====\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "====train acc: 0.9806978470675576 test acc: 0.9622222222222222 ====\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "====train acc: 0.9643652561247216 test acc: 0.9555555555555556 ====\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "====train acc: 0.9755011135857461 test acc: 0.9644444444444444 ====\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "====train acc: 0.9844097995545658 test acc: 0.9733333333333334 ====\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "====train acc: 0.9821826280623608 test acc: 0.9533333333333334 ====\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "====train acc: 0.9851521900519673 test acc: 0.9622222222222222 ====\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "====train acc: 0.9851521900519673 test acc: 0.9622222222222222 ====\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "====train acc: 0.9821826280623608 test acc: 0.9733333333333334 ====\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "====train acc: 0.9866369710467706 test acc: 0.9733333333333334 ====\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "====train acc: 0.9836674090571641 test acc: 0.9622222222222222 ====\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "====train acc: 0.9910913140311804 test acc: 0.9733333333333334 ====\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "====train acc: 0.9888641425389755 test acc: 0.9666666666666667 ====\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "====train acc: 0.9925760950259837 test acc: 0.9777777777777777 ====\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "====train acc: 0.9955456570155902 test acc: 0.9733333333333334 ====\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "====train acc: 0.9948032665181886 test acc: 0.9733333333333334 ====\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "====train acc: 0.994060876020787 test acc: 0.9755555555555555 ====\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "====train acc: 0.9948032665181886 test acc: 0.9711111111111111 ====\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "====train acc: 0.994060876020787 test acc: 0.9711111111111111 ====\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "====train acc: 0.9962880475129918 test acc: 0.9711111111111111 ====\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "====train acc: 0.9955456570155902 test acc: 0.98 ====\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "====train acc: 0.9970304380103935 test acc: 0.9777777777777777 ====\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "====train acc: 0.991833704528582 test acc: 0.9711111111111111 ====\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "====train acc: 0.9970304380103935 test acc: 0.9733333333333334 ====\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "====train acc: 0.9955456570155902 test acc: 0.9755555555555555 ====\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "====train acc: 0.9977728285077951 test acc: 0.9711111111111111 ====\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "====train acc: 0.9925760950259837 test acc: 0.9577777777777777 ====\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "====train acc: 0.9955456570155902 test acc: 0.9688888888888889 ====\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "====train acc: 0.9985152190051967 test acc: 0.9777777777777777 ====\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "====train acc: 0.9948032665181886 test acc: 0.9688888888888889 ====\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "====train acc: 0.9992576095025983 test acc: 0.9688888888888889 ====\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "====train acc: 1.0 test acc: 0.9688888888888889 ====\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "====train acc: 0.9985152190051967 test acc: 0.9755555555555555 ====\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "====train acc: 1.0 test acc: 0.9755555555555555 ====\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "====train acc: 1.0 test acc: 0.9733333333333334 ====\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=64, hidden_size=128, output_size=10)\n",
    "\n",
    "iters_num = 500\n",
    "train_size = X_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = X_train[batch_mask]\n",
    "    print(i)\n",
    "    t_batch = y_train[batch_mask]\n",
    "    \n",
    "    # 勾配\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        train_acc = network.accuracy(X_train, y_train)\n",
    "        test_acc = network.accuracy(X_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"====train acc: \" + str(train_acc), \"test acc: \" + str(test_acc), \"====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1578727056180,
     "user": {
      "displayName": "ちんちゃんねるAIエンジニア",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDAp76sYQh20e_nmKms9FsgMnK6Wln1dGNxxPr_=s64",
      "userId": "02172563712230456611"
     },
     "user_tz": -540
    },
    "id": "A1MzVAQC_ftb",
    "outputId": "a4ab99f0-83fc-4162-a2e3-8d1cd9f9930b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 9]\n"
     ]
    }
   ],
   "source": [
    "pred = network.predict(X_test[:2])\n",
    "print(np.argmax(pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1578726771087,
     "user": {
      "displayName": "ちんちゃんねるAIエンジニア",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDAp76sYQh20e_nmKms9FsgMnK6Wln1dGNxxPr_=s64",
      "userId": "02172563712230456611"
     },
     "user_tz": -540
    },
    "id": "NK3ph0wvcZPG",
    "outputId": "48938ce4-aaba-477b-ccae-8d908fb679fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(X_test[:2]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVh5rokucn2y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "python_vs_tensorflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
